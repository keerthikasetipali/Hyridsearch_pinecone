# -*- coding: utf-8 -*-
"""hybridsearch_pinecone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f5Sh_-gay7Y3gSVwA6GzM5f2gIqLd0q1
"""

!pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29 sentence-transformers langchain -groq --quiet

!pip list | grep crewai_tools

!pip install crewai
!pip install 'crewai[tools]'

!pip install pdfplumber

!pip install PyPDF2

!pip install fpdf

!pip install langchain-groq

!pip install langchain_huggingface

!pip install langchain_openai

!pip install crewai-tools --upgrade

!pip install langchain

!pip install pinecone

!pip install --upgrade --quiet pinecone-client pinecone-text pinecone-notebooks

api_key="d723ac4c-e142-4462-9b2a-9185a255a853"

from langchain_community.retrievers import PineconeHybridSearchRetriever

import os
from pinecone import Pinecone,ServerlessSpec
index_name = "hybrid-search-langchain-pinecone" # Changed index name to use only lowercase letters, numbers, and hyphens
pc=Pinecone(api_key=api_key)
#create the index
if index_name not in pc.list_indexes().names():
  pc.create_index(
    name=index_name,
    dimension=384, # Replace with your model dimensions
    metric='dotproduct', # Replace with your model metric
    spec=ServerlessSpec(
        cloud='aws',
        region="us-east-1"
    )
)

index=pc.Index(index_name)
index

!pip install -q datasets --upgrade
!pip install -q pyarrow --upgrade

from dotenv import load_dotenv
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

!pip uninstall -y datasets
!pip uninstall -y pyarrow
!pip install datasets
!pip install pyarrow

from langchain_huggingface import HuggingFaceEmbeddings
embeddings=HuggingFaceEmbeddings(model_name ="all-MiniLM-L6-v2")
embeddings

from pinecone_text.sparse import BM25Encoder
bm25_encoder = BM25Encoder().default()
bm25_encoder

sentences=[
    "In 2023,I visited Paris ",
    "In 2022,I visited korea ",
    "In 2021,I visited Pakistan ",
]
bm25_encoder.fit(sentences)
bm25_encoder.dump("bm25_values.json")
bm25_encoder = BM25Encoder().load("bm25_values.json")

retriever=PineconeHybridSearchRetriever(
    embeddings=embeddings,
    sparse_encoder=bm25_encoder,
    index=index)

retriever.add_texts(
    [
    "In 2023,I visited Paris ",
    "In 2022,I visited korea ",
    "In 2021,I visited Pakistan ",
]

)

results=retriever.invoke("When did i visited pakistan?")

top_result = results[0] if results else None

print("Top Result:", top_result)

"""NOW FOR THE PDF NOT JUST FOR THE SENTENCES

"""

!pip install pdfplumber

!pip install PyPDF2

!pip install fpdf

import requests
from PyPDF2 import PdfMerger

# List of PDF URLs
pdf_urls = [
    'https://drive.google.com/uc?export=download&id=1i3kfUEXrdmOFfZCLxEfXK3dv9SonsuTQ',
    'https://drive.google.com/uc?export=download&id=19RL92jlL9Zu1mIN44WUsre20Cx-0YTAS',
    'https://drive.google.com/uc?export=download&id=1qgUHkP8GDTE-1ImqQ67kJQfHDRDIVCUs',
    'https://drive.google.com/uc?export=download&id=151m94JPfFzxKWNrUsLNmZDti7DVqjAtD'

]

# Initialize a PdfMerger object
merger = PdfMerger()

# Function to download PDFs and merge them
def download_and_merge_pdfs(urls, output_filename):
    for i, pdf_url in enumerate(urls):
        response = requests.get(pdf_url)
        temp_pdf = f'temp_document_{i+1}.pdf'  # Temporary filename for each PDF
        with open(temp_pdf, 'wb') as f:
            f.write(response.content)
        merger.append(temp_pdf)  # Add the PDF to the merger

    # Write out the merged PDF
    with open(output_filename, 'wb') as f_out:
        merger.write(f_out)

    merger.close()
    print(f'All PDFs merged into: {output_filename}')

# Merge all PDFs into a single file
download_and_merge_pdfs(pdf_urls, 'merged_document.pdf')

!pip install PyMuPDF
import fitz

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with fitz.open(pdf_path) as pdf_document:
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    return text

# Extract text from PDF
pdf_path = '/content/merged_document.pdf'
pdf_text = extract_text_from_pdf(pdf_path)

# Function to split text into chunks
def split_text(text, chunk_size=1000):
    # Split text by paragraphs
    paragraphs = text.split('\n')

    # Combine paragraphs into chunks
    chunks = []
    current_chunk = ""

    for paragraph in paragraphs:
        if len(current_chunk) + len(paragraph) > chunk_size:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += "\n" + paragraph

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

chunks = split_text(pdf_text)

# Train BM25 encoder and add texts to index
bm25_encoder.fit(chunks)
bm25_encoder.dump("bm25_values.json")
bm25_encoder = BM25Encoder().load("bm25_values.json")

# Initialize retriever
retriever = PineconeHybridSearchRetriever(
    embeddings=embeddings,
    sparse_encoder=bm25_encoder,
    index=index
)

retriever.add_texts(chunks)

# Perform the search and get top result
query = "what are the other laboratories   organized quality week at their respective laboratories?"  # Example query
results = retriever.invoke(query)

print(results)

# Assuming results is a list or similar, return only the top result
top_result = results[0] if results else None

print("Top Result:", top_result)

"""now combining the ragtool CREWAI technique to it

"""

!pip install langchain-groq

!pip install langchain_huggingface

!pip install langchain_openai

from langchain_openai import ChatOpenAI
import os
from crewai_tools import PDFSearchTool
from langchain_community.tools.tavily_search import TavilySearchResults
from crewai_tools import tool
from crewai import Crew
from crewai import Task
from crewai import Agent

os.environ['GROQ_API_KEY'] = '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'

# Initialize PDFSearchTool
rag_tool = PDFSearchTool(
    pdf=pdf_path,  # Use the same PDF path for context
    config=dict(
        llm=dict(
            provider="groq",  # Use the Groq provider
            config=dict(
                model="llama3-13b-8192",
                temperature=0.1,
                top_p=0.65,
                stream=False,
                max_tokens=100,
            ),
        ),
        embedder=dict(
            provider="huggingface",
            config=dict(
                model="BAAI/bge-small-en-v1.5",
            ),
        ),
    )
)

# Install required package
!pip install sentence-transformers
from sentence_transformers import SentenceTransformer, util

# Initialize SentenceTransformer model
similarity_model = SentenceTransformer('all-mpnet-base-v2')

# Define the compute_similarity function
def compute_similarity(query, response):
    query_embedding = similarity_model.encode(query, convert_to_tensor=True)
    response_embedding = similarity_model.encode(response, convert_to_tensor=True)
    cosine_sim = util.cos_sim(query_embedding, response_embedding)
    return cosine_sim.item()

def get_specific_answer(query):
    # Perform hybrid search
    hybrid_results = retriever.invoke(query)
    if hybrid_results:
        # Use the top result from hybrid search
        top_result = hybrid_results[0].page_content
    else:
        top_result = None

    if top_result:
        # Run PDFSearchTool for semantic understanding
        response = rag_tool.run(query)
        if not response or "not found" in response.lower():
            return "Can't find the approximate answer."

        # Compute similarity between the query and the response
        similarity_score = compute_similarity(query, response)

        # Set a threshold for similarity score (e.g., 0.4)
        if similarity_score < 0.4:
            return "Can't find the approximate answer."

        return response

    return "No relevant results found from hybrid search."

query = "what are the other laboratories   organized quality week as DRDO QUALITY CONCLAVE 2023?"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query = "who is keerthi?"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query = "briefly tell the successful flight-test of new generation akash missile"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query = "When DRDO celebrated its 66 th foundation day"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query = "Who is your present pm of India"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

"""TRYING FOR THE WEB SEARCH"""

os.environ['TAVILY_API_KEY'] = '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'

tavily_search_tool = TavilySearchResults( k=3)

def web_search(question):
    """Performs a web search using the Tavily API."""

    # Implement the Tavily API call here
    response = tavily_search_tool(question)  # Assuming you have a function for this

    # Return the web search result
    if response:
        return response
    else:
        return "No relevant information found online."

def get_specific_answer(query):
    # Perform hybrid search
    hybrid_results = retriever.invoke(query)
    if hybrid_results:
        # Use the top result from hybrid search
        top_result = hybrid_results[0].page_content
    else:
        top_result = None

    if top_result:
        # Run PDFSearchTool for semantic understanding
        response = rag_tool.run(query)
        if not response or "not found" in response.lower():
            # If PDFSearchTool fails, perform a web search
            return web_search(query)

        # Compute similarity between the query and the response
        similarity_score = compute_similarity(query, response)

        # Set a threshold for similarity score (e.g., 0.4)
        if similarity_score < 0.4:
            return web_search(query)

        return response

    # If hybrid search fails, use PDFSearchTool directly
    response = rag_tool.run(query)
    if not response or "not found" in response.lower():
        # If PDFSearchTool fails, perform a web search
        return web_search(query)

    return response
    relevant_sentence = extract_relevant_sentence(response, query)
    return relevant_sentence

query = "Who is your present pm of India"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query = "Who designed and developed this UAV"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query = "Who designed and developed this UAV"
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

query="which is the first country to demonstrate the capability of engagement of 04 aerial targets simultaneously at 25Km ranges "
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

"""trying for the revelant answer"""

model = SentenceTransformer('all-MiniLM-L6-v2')

def extract_relevant_sentence(text, query):
    # Split the text into sentences
    sentences = text.split('. ')

    # Encode the query and all sentences into embeddings
    query_embedding = model.encode(query, convert_to_tensor=True)
    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)

    # Calculate the cosine similarity between the query and each sentence
    cosine_similarities = util.cos_sim(query_embedding, sentence_embeddings)

    # Find the index of the sentence with the highest similarity score
    best_sentence_index = cosine_similarities.argmax().item()

    # Return the most relevant sentence
    return sentences[best_sentence_index].strip() + '.'

query="which is the first country to demonstrate the capability of engagement of 04 aerial targets simultaneously at 25Km ranges "
answer = get_specific_answer(query)
print(f"Query: {query}\nAnswer: {answer}\n")

"""2nd method using Spacy"""

import spacy

# Load the spaCy model for Named Entity Recognition
nlp = spacy.load("en_core_web_sm")

def extract_relevant_information(text, query):
    # Process the text and query with spaCy
    doc = nlp(text)
    query_doc = nlp(query)

    # Extract named entities from the query and text
    query_entities = [ent.text for ent in query_doc.ents]
    text_entities = [(ent.text, ent.start_char, ent.end_char) for ent in doc.ents]

    # Filter sentences that contain entities similar to those in the query
    sentences = list(doc.sents)
    relevant_sentences = []

    for sentence in sentences:
        for ent in sentence.ents:
            if ent.text in query_entities:
                relevant_sentences.append(sentence.text)
                break

    # If no relevant sentence is found, return the entire text or the first sentence
    if not relevant_sentences:
        return sentences[0].text if sentences else text

    # Return the most contextually relevant sentence
    return " ".join(relevant_sentences).strip()

query="which is the first country to demonstrate the capability of engagement of 04 aerial targets simultaneously at 25Km ranges "
answer = extract_relevant_information(pdf_text,query)
print(f"Query: {query}\nAnswer: {answer}\n")

